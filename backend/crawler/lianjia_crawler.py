import json
import random
import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

try:
    from urllib3.util.retry import Retry
except Exception:
    Retry = None


# ===========================
# é…ç½®åŒº
# ===========================
BASE_URL = "https://bj.lianjia.com/ershoufang/{district}/pg{page}/"

DISTRICTS = {
    "ä¸œåŸŽ": "dongcheng",
    "æµ·æ·€": "haidian",
    "é€šå·ž": "tongzhou",
    "æ€€æŸ”": "huairou",
}

TARGET_PER_DISTRICT = 1000
MAX_PAGES_PER_DISTRICT = 200

# å¼ºçƒˆå»ºè®®ï¼šå…ˆç”¨â€œä»…åˆ—è¡¨æ¨¡å¼â€è·‘æ»¡ 1000/åŒºï¼ˆå¿«ã€ç¨³å®šï¼‰
DETAIL_MODE = False  # True ä¼šéžå¸¸æ…¢ä¸”æ›´æ˜“é£ŽæŽ§ï¼ˆä¸å»ºè®®ï¼‰

# é—´éš”ï¼šåˆ—è¡¨é¡µè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
SLEEP_LIST_MIN = 0.6
SLEEP_LIST_MAX = 1.2

OUTPUT_NAME = "lianjia_housing_beijing_4districts.json"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Connection": "keep-alive",
    "Referer": "https://bj.lianjia.com/",
}

# å¤‡é€‰ï¼šæ‰‹åŠ¨ Cookieï¼ˆä¸å»ºè®®åœ¨ä»»ä½•å…¬å…±ä½ç½®ç²˜è´´ï¼‰
MANUAL_COOKIE_STR = "SECKEY_ABVK=+RobYf0w6e1/x+aAkLdRVNq0jrnp4QMe9r1GSXG7AmU%3D; BMAP_SECKEY=bQDX3Y-AYHFj75g9-g4NljrXDcRes3Q2EqmOB9kTWXtHOofZiDMs8PMKsKi3-BS3th88kKmSEDPdaXo3iP91oTc0EMzlK4h82db7DAlp7jCvg8n38xeTDF_bM7cdX6E6VrR1IQ4PRI_86u0Sw-h44kcQtVCZvxFMgehMKdtGlFxQvr8mwJcLFcwOhd3tyBAp; lianjia_uuid=08684bbe-80f4-46f0-8281-b52134a35ae9; _ga=GA1.2.900769530.1764558432; crosSdkDT2019DeviceId=-17jmvd--t2f7fy-o1nb2mjtbqdrhwe-u424wnkqj; ftkrc_=4185d07a-4ebd-486e-9192-5772e869b7cd; lfrc_=3974f4a2-78f6-4f85-b851-3656f6474ff6; login_ucid=2000000515412377; lianjia_token=2.00135d346d480ec81802f01d5cc40a1f41; lianjia_token_secure=2.00135d346d480ec81802f01d5cc40a1f41; security_ticket=Le6NbzkfDhIqQKEuO4GSOru0kHGmj6HHhMzZaz4aPNnabPfysLJ1NX3l6bFvNSzYjA/GLQLZ/glpqnpUVXyDS/GEdFKE9XWf2G8VGWY33/bFS0EyeNt2Mf5tSgqbJO6+G+xwvI7GEH5S34HSDvAFV4nw1tLyIuh3PVm8QlifLsQ=; _jzqckmp=1; _gid=GA1.2.778516721.1765700918; _ga_RCTBRFLNVS=GS2.2.s1765703621$o1$g0$t1765703621$j60$l0$h0; _ga_LRLL77SF11=GS2.2.s1765703643$o1$g1$t1765703671$j32$l0$h0; _ga_GVYN2J1PCG=GS2.2.s1765703643$o1$g1$t1765703671$j32$l0$h0; _ga_C4R21H79WC=GS2.2.s1765703676$o1$g1$t1765703678$j58$l0$h0; _ga_WLZSQZX7DE=GS2.2.s1765703630$o1$g1$t1765704185$j60$l0$h0; _ga_TJZVFLS7KV=GS2.2.s1765703630$o1$g1$t1765704185$j60$l0$h0; lianjia_ssid=3a94498c-8526-4a71-9793-0629d6dad913; Hm_lvt_46bf127ac9b856df503ec2dbf942b67e=1764809305,1765376715,1765700901,1765705031; HMACCOUNT=4DF22A6659352A8E; _jzqa=1.3421507242892507600.1764558424.1765703116.1765705031.10; _jzqc=1; _jzqx=1.1764558424.1765705031.5.jzqsr=ucloud%2Ebupt%2Eedu%2Ecn|jzqct=/.jzqsr=cn%2Ebing%2Ecom|jzqct=/; _ga_654P0WDKYN=GS2.2.s1765704222$o1$g1$t1765705044$j59$l0$h0; select_city=110000; _qzjc=1; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2219ad7e0cceca47-0049a672629ab2-4c657b58-1638720-19ad7e0ccede8d%22%2C%22%24device_id%22%3A%2219ad7e0cceca47-0049a672629ab2-4c657b58-1638720-19ad7e0ccede8d%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%7D; hip=N9jSPiT4n18zdNyLMgphmiJ6zqFqjrVtl2BFCVcp56QCwMinLi6GDzyW1x4AGmkVXnJnstmKNA-i-mjLvPZ-KcCD309ykSiw1OWAofxX58VrwbE14DLXV-FebZmloXpN4sGkzfB6kI6w5Vvwxq5CphqwFX93RBJxrhXwuIXjmieDvPNXkgdRwOwkRxCAiONWMSP5p08JE8eTdDz8XUFQZmXhkdE2bpcvm9HM1Ve1TDIr3W0oc4ANQa4DHSW3Hy5L5QKAxrzlvWMYnLtnkccU5LqGLBZEeIo3dxHz; _gat=1; _gat_past=1; _gat_global=1; _gat_new_global=1; _gat_dianpu_agent=1; Hm_lpvt_46bf127ac9b856df503ec2dbf942b67e=1765707661; _jzqb=1.15.10.1765705031.1; _qzja=1.654706070.1764088221699.1765703115920.1765705062068.1765707625354.1765707661357.0.0.0.92.11; _qzjb=1.1765703115920.44.0.0.0; _qzjto=71.3.0; srcid=eyJ0Ijoie1wiZGF0YVwiOlwiNzk1ZjdjZjczNGFiYTZiMjkxMzYzMzQwODA4MDg2ZjlmYzFhOGZhNTE0MWZiYTQyZmM0OGE3OWMzYzY3YjI3YmM3ZDJlZWY4ZTVmMTYwNmQwNjgyMGUyODNhYTcyNjVmYzJmMGE2ZmEyYTMyZjBlNDc0ZDEyZDg2M2EyOTBhNzA3MmNhN2UzYzM0NzlhNWYyNDI5NGVjN2E0MDU4MmQ0NGU1YTdmYjkwNmRlYmFmMzAwYzUyOWMyNTc4MzdjNmNkMTI2NTVlZDFkYWQ4OTc2YzliZTZlMGFlODM0NWViNzQ1ZWRiY2MzZDIxYmQ2YzJiZmMyODQyM2U2ODc1ZjljZVwiLFwia2V5X2lkXCI6XCIxXCIsXCJzaWduXCI6XCJiNGU1NmEwOVwifSIsInIiOiJodHRwczovL2JqLmxpYW5qaWEuY29tL2Vyc2hvdWZhbmcvZG9uZ2NoZW5nL3BnMS8iLCJvcyI6IndlYiIsInYiOiIwLjEifQ==; _ga_KJTRWRHDL1=GS2.2.s1765703138$o9$g1$t1765707672$j24$l0$h0; _ga_QJN1VP0CMS=GS2.2.s1765703138$o9$g1$t1765707672$j24$l0$h0"


# ===========================
# è·¯å¾„ï¼ˆä¿®å¤ debug_html æŠ¥é”™ï¼šç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ç»å¯¹è·¯å¾„ï¼‰
# ===========================
BASE_DIR = Path(__file__).resolve().parent
DEBUG_DIR = BASE_DIR / "debug_html"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_FILE = BASE_DIR / OUTPUT_NAME


def dump_html(tag: str, region: str, page: int, url: str, html: str):
    try:
        DEBUG_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        p = DEBUG_DIR / f"{ts}_{tag}_{region}_pg{page}.html"
        p.write_text(html or "", encoding="utf-8", errors="ignore")
        print(f"  ðŸ§¾ å·²ä¿å­˜è°ƒè¯•é¡µé¢ï¼š{p}")
        print(f"  ðŸ”— URL: {url}")
    except Exception as e:
        # ä¸å…è®¸è°ƒè¯•è½ç›˜å¯¼è‡´ç¨‹åºå´©æºƒ
        print(f"  âš  è°ƒè¯•é¡µé¢ä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼Œä¸ä¸­æ–­ï¼‰ï¼š{e}")


# ===========================
# Cookie
# ===========================
def parse_manual_cookie_str(cookie_str: str) -> Dict[str, str]:
    cookie_str = cookie_str.strip().strip(";")
    if not cookie_str:
        return {}
    cookies: Dict[str, str] = {}
    for p in cookie_str.split(";"):
        p = p.strip()
        if "=" not in p:
            continue
        k, v = p.split("=", 1)
        cookies[k.strip()] = v.strip()
    return cookies


def load_browser_cookies(domain: str = ".lianjia.com"):
    try:
        import browser_cookie3
    except Exception:
        return None

    for name in ("edge", "chrome", "firefox"):
        fn = getattr(browser_cookie3, name, None)
        if not fn:
            continue
        try:
            cj = fn(domain_name=domain)
            if cj and len(cj) > 0:
                print(f"âœ… å·²ä»Žæµè§ˆå™¨è¯»å– Cookieï¼š{name}ï¼ˆæ¡ç›®æ•°ï¼š{len(cj)}ï¼‰")
                return cj
        except Exception:
            continue
    return None


def get_cookies() -> Union[Dict[str, str], requests.cookies.RequestsCookieJar, None]:
    if MANUAL_COOKIE_STR.strip():
        manual = parse_manual_cookie_str(MANUAL_COOKIE_STR)
        print("âœ… ä½¿ç”¨æ‰‹åŠ¨ç²˜è´´çš„ Cookieï¼Œæ¡ç›®æ•°ï¼š", len(manual))
        return manual

    cj = load_browser_cookies(".lianjia.com")
    if cj:
        return cj

    print("âš  æœªèŽ·å–åˆ° Cookieã€‚å»ºè®®ï¼šå…ˆç™»å½•é“¾å®¶ç½‘é¡µç‰ˆå†è¿è¡Œï¼›æˆ–æŠŠ Cookie ç²˜è´´åˆ° MANUAL_COOKIE_STRã€‚")
    return None


# ===========================
# HTML & é£ŽæŽ§åˆ¤æ–­ï¼ˆåªåœ¨â€œè§£æžä¸åˆ°æˆ¿æºâ€æ—¶æ‰è®¤å®šé£ŽæŽ§ï¼‰
# ===========================
def soup_of(html: str) -> BeautifulSoup:
    try:
        return BeautifulSoup(html, "lxml")
    except Exception:
        return BeautifulSoup(html, "html.parser")


def looks_like_verify_page(html: str) -> bool:
    if not html:
        return True
    s = html
    soup = soup_of(html)
    title = soup.title.get_text(strip=True) if soup.title else ""

    hard_signals = [
        "è®¿é—®éªŒè¯", "å®‰å…¨éªŒè¯", "äººæœºéªŒè¯", "éªŒè¯ç ", "å¼‚å¸¸è®¿é—®", "æ“ä½œå¤ªé¢‘ç¹",
        "ke-passport", "ç™»å½•é“¾å®¶", "é“¾å®¶ç½‘ç”¨æˆ·ç™»å½•",
    ]
    has_signal = any(x in s for x in hard_signals) or any(x in title for x in hard_signals)

    has_list = soup.find("ul", class_="sellListContent") is not None
    return (not has_list) and has_signal


def polite_sleep(lo: float, hi: float):
    time.sleep(random.uniform(lo, hi))


# ===========================
# è§£æžï¼šåˆ—è¡¨é¡µå­—æ®µï¼ˆæ»¡è¶³ä½œä¸šå­—æ®µï¼‰
# ===========================
@dataclass
class ListRow:
    region: str
    district_slug: str
    name: Optional[str]
    bizcircle: Optional[str]
    total_price_wan: Optional[float]
    unit_price_yuan_sqm: Optional[int]
    layout: Optional[str]
    room_count: Optional[int]
    hall_count: Optional[int]
    area_sqm: Optional[float]
    orientation: Optional[str]
    decoration: Optional[str]
    floor: Optional[str]
    building_year: Optional[int]
    building_age: Optional[int]
    elevator: Optional[str]
    deal_or_publish_time: Optional[str]
    publish_time: Optional[str]
    detail_url: Optional[str]
    house_id: Optional[str]
    tags: List[str]


def extract_house_id(url: str) -> Optional[str]:
    m = re.search(r"/ershoufang/(\d+)\.html", url)
    return m.group(1) if m else None


def parse_publish_date_from_followinfo(text: str) -> Optional[str]:
    if not text:
        return None
    t = text.strip()

    # å¸¸è§ï¼š"... / 7å¤©ä»¥å‰å‘å¸ƒ"ã€"... / 2ä¸ªæœˆä»¥å‰å‘å¸ƒ"ã€"... / åˆšåˆšå‘å¸ƒ"
    if "åˆšåˆšå‘å¸ƒ" in t:
        return datetime.now().strftime("%Y-%m-%d")

    m_day = re.search(r"(\d+)\s*å¤©ä»¥å‰å‘å¸ƒ", t)
    if m_day:
        days = int(m_day.group(1))
        d = (datetime.now() - timedelta(days=days)).date()
        return d.strftime("%Y-%m-%d")

    m_month = re.search(r"(\d+)\s*ä¸ªæœˆä»¥å‰å‘å¸ƒ", t)
    if m_month:
        months = int(m_month.group(1))
        d = (datetime.now() - timedelta(days=30 * months)).date()
        return d.strftime("%Y-%m-%d")

    m_year = re.search(r"(\d+)\s*å¹´ä»¥å‰å‘å¸ƒ", t)
    if m_year:
        years = int(m_year.group(1))
        d = (datetime.now() - timedelta(days=365 * years)).date()
        return d.strftime("%Y-%m-%d")

    return None


def calc_building_age(building_year: Optional[int]) -> Optional[int]:
    if not building_year:
        return None
    try:
        return max(0, datetime.now().year - int(building_year))
    except Exception:
        return None


def parse_list_page(html: str, region_cn: str, district_slug: str) -> List[Dict]:
    soup = soup_of(html)
    container = soup.find("ul", class_="sellListContent")
    if not container:
        return []

    rows: List[Dict] = []

    for li in container.find_all("li", class_="clear"):
        try:
            # è¯¦æƒ…é“¾æŽ¥ & id
            title_div = li.find("div", class_="title")
            a = title_div.find("a") if title_div else None
            detail_url = a["href"].strip() if a and a.get("href") else None
            house_id = extract_house_id(detail_url) if detail_url else None

            # æ¥¼ç›˜ / å•†åœˆ
            name = None
            bizcircle = None
            pos_info = li.find("div", class_="positionInfo")
            if pos_info:
                a_tags = pos_info.find_all("a")
                if len(a_tags) >= 1:
                    name = a_tags[0].get_text(strip=True)
                if len(a_tags) >= 2:
                    bizcircle = a_tags[1].get_text(strip=True)
            if not name and a:
                name = a.get_text(strip=True)

            # æ€»ä»·
            total_price_wan = None
            total_div = li.find("div", class_="totalPrice")
            if total_div and total_div.span:
                try:
                    total_price_wan = float(total_div.span.get_text(strip=True))
                except Exception:
                    total_price_wan = None

            # å•ä»·
            unit_price_yuan_sqm = None
            unit_div = li.find("div", class_="unitPrice")
            if unit_div:
                unit_text = unit_div.get_text(strip=True)
                m_unit = re.search(r"([\d,]+)", unit_text)
                if m_unit:
                    try:
                        unit_price_yuan_sqm = int(m_unit.group(1).replace(",", ""))
                    except Exception:
                        unit_price_yuan_sqm = None

            # houseInfoï¼šæˆ·åž‹/é¢ç§¯/æœå‘/è£…ä¿®/æ¥¼å±‚/å¹´ä»½ç­‰
            layout = None
            room_count = None
            hall_count = None
            area_sqm = None
            orientation = None
            decoration = None
            floor = None
            building_year = None
            elevator = None  # åˆ—è¡¨é¡µé€šå¸¸å–ä¸åˆ°ï¼Œåšå¼±åŒ¹é…/ç•™ç©º

            house_info_div = li.find("div", class_="houseInfo")
            info_text = ""
            if house_info_div:
                info_text = house_info_div.get_text(separator="|", strip=True)
                parts = [p.strip() for p in info_text.split("|") if p.strip()]

                if parts:
                    layout = parts[0]
                    m_room = re.search(r"(\d+)\s*å®¤", layout)
                    m_hall = re.search(r"(\d+)\s*åŽ…", layout)
                    if m_room:
                        room_count = int(m_room.group(1))
                    if m_hall:
                        hall_count = int(m_hall.group(1))

                for p in parts:
                    if "å¹³ç±³" in p:
                        m_area = re.search(r"([\d.]+)", p)
                        if m_area:
                            area_sqm = float(m_area.group(1))
                        break

                for p in parts:
                    if re.fullmatch(r"[ä¸œå—è¥¿åŒ—]{1,4}", p):
                        orientation = p
                        break

                for p in parts:
                    if p in ("ç²¾è£…", "ç®€è£…", "æ¯›å¯", "å…¶ä»–"):
                        decoration = p
                        break

                for p in parts:
                    if "æ¥¼å±‚" in p or ("å±‚" in p and "å…±" in p):
                        floor = p
                        break

                for p in parts:
                    m_year = re.search(r"(\d{4})\s*å¹´(?:å»º)?", p)
                    if m_year:
                        building_year = int(m_year.group(1))
                        break

            # ç”µæ¢¯å¼±åŒ¹é…ï¼ˆèƒ½å–åˆ°å°±å–ï¼›å–ä¸åˆ°ä¸å¼ºæ±‚ï¼‰
            if info_text:
                if "æœ‰ç”µæ¢¯" in info_text:
                    elevator = "æœ‰"
                elif "æ— ç”µæ¢¯" in info_text:
                    elevator = "æ— "

            # æ ‡ç­¾
            tags: List[str] = []
            tag_div = li.find("div", class_="tag")
            if tag_div:
                for s in tag_div.find_all(["span", "a"]):
                    t = s.get_text(strip=True)
                    if t:
                        tags.append(t)
            if elevator is None and tags:
                if any("ç”µæ¢¯" in t for t in tags):
                    elevator = "æœ‰"

            # å‘å¸ƒæ—¶é—´ï¼ˆåœ¨å”®ç”¨å‘å¸ƒæ—¶é—´å……å½“â€œæˆäº¤æ—¶é—´å­—æ®µâ€ï¼‰
            publish_time = None
            follow = li.find("div", class_="followInfo")
            if follow:
                follow_text = follow.get_text(" ", strip=True)
                publish_time = parse_publish_date_from_followinfo(follow_text)

            deal_or_publish_time = publish_time  # åœ¨å”®ï¼šç”¨å‘å¸ƒæ—¶é—´

            row = ListRow(
                region=region_cn,
                district_slug=district_slug,
                name=name,
                bizcircle=bizcircle,
                total_price_wan=total_price_wan,
                unit_price_yuan_sqm=unit_price_yuan_sqm,
                layout=layout,
                room_count=room_count,
                hall_count=hall_count,
                area_sqm=area_sqm,
                orientation=orientation,
                decoration=decoration,
                floor=floor,
                building_year=building_year,
                building_age=calc_building_age(building_year),
                elevator=elevator,  # å¯èƒ½ä¸º None
                deal_or_publish_time=deal_or_publish_time,
                publish_time=publish_time,
                detail_url=detail_url,
                house_id=house_id,
                tags=tags,
            )

            # è¾“å‡ºå­—æ®µï¼ˆâ‰¥10ï¼Œå¹¶åŒ…å«ä½ è¦æ±‚çš„é‚£äº›å­—æ®µåï¼‰
            rows.append({
                "region": row.region,
                "district_slug": row.district_slug,
                "name": row.name,
                "bizcircle": row.bizcircle,

                "total_price_wan": row.total_price_wan,
                "unit_price_yuan_sqm": row.unit_price_yuan_sqm,

                "layout": row.layout,
                "room_count": row.room_count,
                "hall_count": row.hall_count,
                "area_sqm": row.area_sqm,
                "orientation": row.orientation,
                "building_year": row.building_year,
                "building_age": row.building_age,
                "decoration": row.decoration,
                "floor": row.floor,
                "elevator": row.elevator,

                "deal_or_publish_time": row.deal_or_publish_time,
                "publish_time": row.publish_time,

                "detail_url": row.detail_url,
                "house_id": row.house_id,
                "tags": row.tags,
            })

        except Exception:
            continue

    return rows


# ===========================
# ç½‘ç»œ Session
# ===========================
def build_session(cookies) -> requests.Session:
    s = requests.Session()
    if Retry is not None:
        retry = Retry(
            total=4,
            backoff_factor=0.7,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=("GET",),
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
        s.mount("https://", adapter)
        s.mount("http://", adapter)

    if cookies:
        try:
            s.cookies.update(cookies)
        except Exception:
            pass
    return s


def safe_get(session: requests.Session, url: str, timeout: int = 25) -> Optional[requests.Response]:
    try:
        resp = session.get(url, headers=HEADERS, timeout=timeout)
        resp.encoding = "utf-8"
        return resp
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")
        return None


# ===========================
# çˆ¬å–ï¼šä»…åˆ—è¡¨ï¼ˆå¿« + ä¸æ˜“é£ŽæŽ§ï¼‰
# ===========================
def crawl_district_list_only(session: requests.Session, district_cn: str, district_slug: str) -> List[Dict]:
    collected: List[Dict] = []
    seen_ids = set()

    for page in range(1, MAX_PAGES_PER_DISTRICT + 1):
        if len(collected) >= TARGET_PER_DISTRICT:
            break

        url = BASE_URL.format(district=district_slug, page=page)
        print(f"\n[{district_cn}] æŠ“å–åˆ—è¡¨é¡µï¼šç¬¬ {page} é¡µ  | å½“å‰å·²æ”¶é›†ï¼š{len(collected)}")
        resp = safe_get(session, url)
        if not resp:
            break

        print("  çŠ¶æ€ç :", resp.status_code)
        if resp.status_code != 200:
            dump_html("list_non200", district_slug, page, url, resp.text)
            break

        rows = parse_list_page(resp.text, district_cn, district_slug)

        if not rows:
            if looks_like_verify_page(resp.text):
                print("  âš  ç–‘ä¼¼è¢«é£ŽæŽ§/éªŒè¯é¡µæ‹¦æˆªï¼ˆåˆ—è¡¨é¡µæ— æˆ¿æºç»“æž„ï¼‰ã€‚")
                dump_html("list_verify", district_slug, page, url, resp.text)
                # é€€é¿ç­‰å¾…ä¸€æ¬¡å†é‡è¯•åŒé¡µï¼ˆä¸ç»•è¿‡éªŒè¯ï¼Œåªé™ä½Žè§¦å‘é¢‘çŽ‡ï¼‰
                wait = random.uniform(18, 35)
                print(f"  â³ é€€é¿ç­‰å¾… {wait:.1f}s åŽé‡è¯•åŒé¡µä¸€æ¬¡...")
                time.sleep(wait)

                resp2 = safe_get(session, url)
                if resp2 and resp2.status_code == 200:
                    rows2 = parse_list_page(resp2.text, district_cn, district_slug)
                    if rows2:
                        rows = rows2
                    else:
                        dump_html("list_verify_retry_fail", district_slug, page, url, resp2.text)
                        break
                else:
                    break
            else:
                print("  âš  åˆ—è¡¨é¡µè§£æžä¸åˆ°æˆ¿æºï¼ˆå¯èƒ½ç»“æž„å˜æ›´æˆ–ç¡®å®žæ— æ•°æ®ï¼‰ã€‚")
                dump_html("list_empty", district_slug, page, url, resp.text)
                break

        print(f"  âœ… åˆ—è¡¨é¡µè§£æžæˆ¿æºæ•°ï¼š{len(rows)}")

        for r in rows:
            hid = r.get("house_id") or r.get("detail_url")
            if hid and hid in seen_ids:
                continue
            if hid:
                seen_ids.add(hid)
            collected.append(r)
            if len(collected) >= TARGET_PER_DISTRICT:
                break

        polite_sleep(SLEEP_LIST_MIN, SLEEP_LIST_MAX)

    return collected


def main():
    cookies = get_cookies()
    session = build_session(cookies)

    all_data: List[Dict] = []
    per = {}

    for district_cn, district_slug in DISTRICTS.items():
        rows = crawl_district_list_only(session, district_cn, district_slug)
        per[district_cn] = len(rows)
        all_data.extend(rows)

        if len(rows) < TARGET_PER_DISTRICT:
            print(f"\nâš  [{district_cn}] ä»…æŠ“åˆ° {len(rows)} æ¡ã€‚è‹¥ debug_html æ˜¯éªŒè¯é¡µï¼Œéœ€è¦é™ä½Žé¢‘çŽ‡æˆ–æ”¹ç”¨äººå·¥æµè§ˆå™¨æ–¹å¼ã€‚")
        else:
            print(f"\nâœ… [{district_cn}] å®Œæˆï¼š{len(rows)} æ¡ï¼ˆâ‰¥{TARGET_PER_DISTRICT}ï¼‰")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print("\nðŸŽ‰ å…¨éƒ¨å®Œæˆï¼")
    print("å„åŒºæ•°é‡ï¼š", per)
    print(f"å·²ä¿å­˜ {len(all_data)} æ¡åˆ°ï¼š{OUTPUT_FILE}")


if __name__ == "__main__":
    main()
