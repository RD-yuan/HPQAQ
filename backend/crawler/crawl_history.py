"""
æˆ¿å¤©ä¸‹å†å²æˆäº¤æ•°æ®çˆ¬è™«
ç‰¹æ€§ï¼š
- æ–­ç‚¹ç»­çˆ¬ï¼š<è„šæœ¬åŒå>.checkpoint.json
- æ•°æ®è½ç›˜ï¼š<è„šæœ¬åŒå>.jsonï¼ˆæ¯é¡µ/è§¦å‘éªŒè¯/Ctrl+C éƒ½ä¼šä¿å­˜ï¼‰
- è§¦å‘é£æ§/éªŒè¯é¡µï¼šæš‚åœ â†’ æ‰‹åŠ¨éªŒè¯ â†’ å¤åˆ¶æœ€æ–° Cookie â†’ é‡è¯•ç»§ç»­çˆ¬
- è¯†åˆ«"è¶…é¡µ/æ— ç»“æœé¡µ"ï¼šè‡ªåŠ¨ç»“æŸè¯¥åŒº

ä¾èµ–ï¼š
pip install -U requests beautifulsoup4 lxml
"""

import json
import random
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

try:
    from urllib3.util.retry import Retry
except Exception:
    Retry = None


# ===========================
# é…ç½®åŒº
# ===========================

# å•†åœˆæˆäº¤è®°å½•URLæ¨¡æ¿ï¼ˆæŒ‰å•†åœˆçˆ¬å–å†å²æˆäº¤æ•°æ®ï¼‰
# æ ¼å¼ï¼šhttps://sh.esf.fang.com/chengjiao-{district}-{bizcircle}/i3{page}/
# æ³¨æ„ï¼šsh è¡¨ç¤ºä¸Šæµ·ï¼Œä¿®æ”¹åŸå¸‚éœ€è¦æ”¹ä¸ºå¯¹åº”åŸå¸‚æ‹¼éŸ³ç¼©å†™ï¼Œå¦‚ bj=åŒ—äº¬, sz=æ·±åœ³
BIZCIRCLE_CHENGJIAO_URL = "https://sh.esf.fang.com/chengjiao-{district}-{bizcircle}/"

# åŒºåŸŸæˆäº¤é¦–é¡µURLæ¨¡æ¿ï¼ˆç”¨äºè·å–è¯¥åŒºåŸŸä¸‹çš„å•†åœˆåˆ—è¡¨ï¼‰
DISTRICT_CHENGJIAO_URL = "https://sh.esf.fang.com/chengjiao-{district}/"

# è¦çˆ¬å–çš„åŒºåŸŸåˆ—è¡¨ï¼šæ¯é¡¹ä¸º (åŒºåŸŸä¸­æ–‡å, åŒºåŸŸä»£ç )
# åŒºåŸŸä»£ç éœ€è¦ä»æˆ¿å¤©ä¸‹ç½‘ç«™URLä¸­è·å–ï¼Œä¸åŒåŸå¸‚çš„åŒºåŸŸä»£ç ä¸åŒ
# ç¤ºä¾‹ï¼šä¸Šæµ·æµ¦ä¸œçš„URLæ˜¯ https://sh.esf.fang.com/chengjiao-a025/ï¼Œåˆ™ä»£ç ä¸º "a025"
# å¦‚éœ€ä¿®æ”¹åŸå¸‚ï¼Œè¯·è®¿é—®ç›®æ ‡åŸå¸‚çš„æˆ¿å¤©ä¸‹äºŒæ‰‹æˆ¿é¡µé¢ï¼ŒæŸ¥çœ‹å„åŒºåŸŸURLä¸­çš„ä»£ç 
DISTRICTS: List[Tuple[str, str]] = [
    ("æµ¦ä¸œ", "a025"),
    # ("å¾æ±‡", "a019"),
    # ("é—µè¡Œ", "a018"),
    # ("é™å®‰", "a021"),
    # ("è™¹å£", "a023"),
    # ("é•¿å®", "a020"),
    # ("å®å±±", "a030"),
]

TARGET_BIZCIRCLES_PER_DISTRICT = 1   # æ¯ä¸ªåŒºåŸŸçˆ¬å–çš„å•†åœˆæ•°é‡ï¼ˆå¯è°ƒæ•´ä»¥æ§åˆ¶æ•°æ®é‡ï¼‰
MAX_PAGES_PER_BIZCIRCLE = 50          # æ¯ä¸ªå•†åœˆæœ€å¤šçˆ¬å–çš„é¡µæ•°ï¼ˆæ¯é¡µçº¦20æ¡æˆäº¤è®°å½•ï¼‰
MAX_EMPTY_PAGES = 12                   # è¿ç»­å¤šå°‘é¡µæ— æ•°æ®ååœæ­¢è¯¥å•†åœˆï¼ˆé¿å…æ— é™çˆ¬å–ç©ºé¡µï¼‰
START_PAGE = 1                        # èµ·å§‹é¡µç ï¼ˆé€šå¸¸ä¸º1ï¼Œé™¤ééœ€è¦è·³è¿‡å‰å‡ é¡µï¼‰

# å¹´ä»½è¿‡æ»¤è®¾ç½®ï¼ˆåªçˆ¬å–æŒ‡å®šå¹´ä»½èŒƒå›´å†…çš„æ•°æ®ï¼‰
MIN_YEAR = 2023                       # æœ€æ—©å¹´ä»½ï¼ˆåŒ…å«ï¼‰ï¼Œæ—©äºæ­¤å¹´ä»½çš„æ•°æ®ä¼šè¢«è·³è¿‡å¹¶åœæ­¢è¯¥å•†åœˆ
MAX_YEAR = 2025                       # æœ€æ™šå¹´ä»½ï¼ˆåŒ…å«ï¼‰ï¼Œæ™šäºæ­¤å¹´ä»½çš„æ•°æ®ä¼šè¢«è·³è¿‡ä½†ç»§ç»­çˆ¬å–

SLEEP_MIN = 1.5                       # è¯·æ±‚é—´éš”æœ€å°ç§’æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡å¿«è¢«å°ï¼‰
SLEEP_MAX = 3.0                       # è¯·æ±‚é—´éš”æœ€å¤§ç§’æ•°ï¼ˆéšæœºå»¶è¿Ÿåœ¨æ­¤èŒƒå›´å†…ï¼‰
SAVE_EVERY_PAGES = 1                  # æ¯çˆ¬å–å¤šå°‘é¡µä¿å­˜ä¸€æ¬¡æ•°æ®ï¼ˆ1è¡¨ç¤ºæ¯é¡µéƒ½ä¿å­˜ï¼‰

VERIFY_KEYWORDS = [
    "è®¿é—®éªŒè¯", "å®‰å…¨éªŒè¯", "äººæœºéªŒè¯", "éªŒè¯ç ", "å¼‚å¸¸è®¿é—®", "æ“ä½œå¤ªé¢‘ç¹", "ç³»ç»Ÿç¹å¿™",
    "è¯·è¾“å…¥éªŒè¯ç ", "æ»‘åŠ¨éªŒè¯",
]

END_KEYWORDS = [
    "æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æˆ¿æº",
    "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æˆ¿æº",
    "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°",
    "æš‚æ— ç›¸å…³æˆ¿æº",
    "æš‚æ— æˆ¿æº",
    "no-result",
    "noresult",
    "noResult",
]

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Connection": "keep-alive",
    "Referer": "https://sh.esf.fang.com/",
}


# ===========================
# è¾“å‡ºï¼šåŒå json + checkpoint + debug_html
# ===========================
BASE_DIR = Path(__file__).resolve().parent
STEM = Path(__file__).stem
OUTPUT_FILE = BASE_DIR / f"{STEM}.json"
CHECKPOINT_FILE = BASE_DIR / f"{STEM}.checkpoint.json"
DEBUG_DIR = BASE_DIR / f"{STEM}_debug_html"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)


# ===========================
# åŸå­å†™æ–‡ä»¶
# ===========================
def atomic_write_text(path: Path, text: str):
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(text or "", encoding="utf-8", errors="ignore")
    tmp.replace(path)


def atomic_write_json(path: Path, obj):
    atomic_write_text(path, json.dumps(obj, ensure_ascii=False, indent=2))


def dump_html(tag: str, district_slug: str, page: int, url: str, html: str):
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        p = DEBUG_DIR / f"{ts}_{tag}_{district_slug}_pg{page}.html"
        atomic_write_text(p, html or "")
        print(f"  ğŸ§¾ å·²ä¿å­˜è°ƒè¯•é¡µé¢ï¼š{p}")
        print(f"  ğŸ”— URL: {url}")
    except Exception as e:
        print(f"  âš  è°ƒè¯•é¡µé¢ä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")


# ===========================
# æ–­ç‚¹ / æ•°æ®è½ç›˜
# ===========================
def load_json(path: Path, default):
    if not path.exists():
        return default
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return default


def save_data(data: List[Dict]):
    try:
        atomic_write_json(OUTPUT_FILE, data)
        print(f"  ğŸ’¾ å·²ä¿å­˜ï¼š{len(data)} æ¡ -> {OUTPUT_FILE.name}")
    except Exception as e:
        print(f"  âš  ä¿å­˜æ•°æ®å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")


def save_checkpoint(current_district_slug: str, next_page: int, stats: Dict[str, int]):
    ck = {
        "current_district_slug": current_district_slug,
        "next_page": next_page,
        "stats": stats,
        "ts": datetime.now().isoformat(),
    }
    try:
        atomic_write_json(CHECKPOINT_FILE, ck)
        print(f"  ğŸ§· å·²ä¿å­˜æ–­ç‚¹ï¼š{CHECKPOINT_FILE.name}")
    except Exception as e:
        print(f"  âš  ä¿å­˜æ–­ç‚¹å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")


# ===========================
# Cookie æ‰‹åŠ¨ç²˜è´´
# ===========================
def parse_manual_cookie_str(cookie_str: str) -> Dict[str, str]:
    cookie_str = cookie_str.strip().strip(";")
    if not cookie_str:
        return {}
    cookies: Dict[str, str] = {}
    for p in cookie_str.split(";"):
        p = p.strip()
        if "=" not in p:
            continue
        k, v = p.split("=", 1)
        cookies[k.strip()] = v.strip()
    return cookies


def prompt_cookie_and_update_session(session: requests.Session) -> bool:
    print("\n================ æ‰‹åŠ¨æ›´æ–° Cookie ================")
    print("æµè§ˆå™¨å®ŒæˆäººæœºéªŒè¯åï¼šF12 â†’ Network â†’ åˆ·æ–°é¡µé¢ â†’ ç‚¹å¼€è¯·æ±‚ â†’")
    print("Request Headers é‡Œå¤åˆ¶æ•´è¡Œ Cookieï¼ˆå•è¡Œï¼‰â†’ ç²˜è´´åˆ°è¿™é‡Œã€‚")
    print("ç›´æ¥å›è½¦è¡¨ç¤ºæ”¾å¼ƒï¼ˆåœæ­¢å½“å‰ä»»åŠ¡ï¼‰ã€‚")
    print("================================================\n")

    cookie_str = input("ç²˜è´´ Cookieï¼ˆå•è¡Œï¼‰> ").strip()
    if not cookie_str:
        return False

    cookie_dict = parse_manual_cookie_str(cookie_str)
    if not cookie_dict:
        print("âš  Cookie è§£æå¤±è´¥ï¼šæ²¡æœ‰è§£æå‡ºä»»ä½• k=vã€‚")
        return False

    try:
        session.cookies.clear()
    except Exception:
        pass
    session.cookies.update(cookie_dict)

    print(f"âœ… Cookie å·²æ›´æ–°ï¼šæ¡ç›®æ•° = {len(cookie_dict)}ï¼ˆå†…å®¹å·²éšè—ï¼‰")
    return True


# ===========================
# Session / è¯·æ±‚é‡è¯•
# ===========================
def build_session() -> requests.Session:
    s = requests.Session()
    if Retry is not None:
        retry = Retry(
            total=4,
            backoff_factor=0.8,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=("GET",),
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
        s.mount("https://", adapter)
        s.mount("http://", adapter)
    return s


def safe_get(session: requests.Session, url: str, timeout: int = 25) -> Optional[requests.Response]:
    try:
        resp = session.get(url, headers=HEADERS, timeout=timeout)
        resp.encoding = "utf-8"
        return resp
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")
        return None


def polite_sleep():
    time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))


# ===========================
# è§£æ & é¡µé¢åˆ†ç±»
# ===========================
def soup_of(html: str) -> BeautifulSoup:
    try:
        return BeautifulSoup(html, "lxml")
    except Exception:
        return BeautifulSoup(html, "html.parser")


def has_deal_list(html: str) -> bool:
    soup = soup_of(html)
    return soup.find("div", class_="houseList") is not None


def get_bizcircles_from_district(html: str, district_code: str) -> List[Tuple[str, str]]:
    """ä»åŒºåŸŸæˆäº¤é¦–é¡µæå–å•†åœˆåˆ—è¡¨ï¼Œåªæå–å½“å‰åŒºåŸŸçš„å•†åœˆ"""
    soup = soup_of(html)
    bizcircles = []
    
    # æŸ¥æ‰¾å•†åœˆé“¾æ¥ï¼Œæ ¼å¼å¦‚ï¼š/chengjiao-a025-b01646/
    links = soup.find_all("a", href=True)
    for link in links:
        href = link.get("href", "")
        # åŒ¹é…å•†åœˆURLæ ¼å¼ï¼š/chengjiao-{district}-{bizcircle}/
        # åªæå–å½“å‰åŒºåŸŸçš„å•†åœˆï¼ˆdistrict_codeå¿…é¡»åŒ¹é…ï¼‰
        pattern = rf"/chengjiao-{re.escape(district_code)}-([a-z0-9]+)/?$"
        m = re.search(pattern, href)
        if m:
            bizcircle_code = m.group(1)
            bizcircle_name = link.get_text(strip=True)
            if bizcircle_name and len(bizcircle_name) > 1:
                bizcircles.append((bizcircle_name, bizcircle_code))
    
    # å»é‡
    seen = set()
    unique_bizcircles = []
    for name, code in bizcircles:
        if code not in seen:
            seen.add(code)
            unique_bizcircles.append((name, code))
    
    return unique_bizcircles


def looks_like_verify_page(html: str) -> bool:
    soup = soup_of(html)
    title = soup.title.get_text(strip=True) if soup.title else ""
    text = (title or "") + " " + (html or "")
    return any(k in text for k in VERIFY_KEYWORDS)


def looks_like_end_page(html: str, page: int) -> bool:
    soup = soup_of(html)
    title = soup.title.get_text(strip=True) if soup.title else ""
    text = (title or "") + " " + (html or "")

    if any(k.lower() in text.lower() for k in END_KEYWORDS):
        return True

    if soup.find(attrs={"class": re.compile(r"no[-_]?result", re.I)}):
        return True

    return False


def classify_page(html: str, page: int) -> str:
    if has_deal_list(html):
        return "OK"

    if looks_like_end_page(html, page):
        return "END"

    if looks_like_verify_page(html):
        return "VERIFY"

    return "UNKNOWN_EMPTY"


# ===========================
# åˆ—è¡¨é¡µè§£æï¼ˆæˆ¿å¤©ä¸‹å†å²æˆäº¤ï¼‰
# ===========================
def parse_deal_date(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"(\d{4})[å¹´\-/](\d{1,2})[æœˆ\-/](\d{1,2})", text)
    if m:
        try:
            return f"{m.group(1)}-{m.group(2).zfill(2)}-{m.group(3).zfill(2)}"
        except Exception:
            return None
    return None


def parse_bizcircle_deals(html: str, bizcircle_name: str, district_cn: str) -> List[Dict]:
    soup = soup_of(html)
    
    rows: List[Dict] = []
    container = soup.find("div", class_="houseList")
    if not container:
        return rows
    
    for item in container.find_all("dl"):
        try:
            # æŸ¥æ‰¾ddæ ‡ç­¾ï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰
            dd = item.find("dd", class_="info")
            if not dd:
                continue
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = dd.find("p", class_="title")
            if not title_elem:
                continue
            
            title_link = title_elem.find("a")
            if not title_link:
                continue
            
            title_text = title_link.get_text(strip=True)
            detail_url = title_link.get("href", "").strip()
            
            house_id = None
            if detail_url:
                m = re.search(r"/chengjiao/(\d+)_", detail_url)
                if m:
                    house_id = m.group(1)
            
            # ä»æ ‡é¢˜æå–å°åŒºåç§°ã€æˆ·å‹å’Œé¢ç§¯
            # æ ‡é¢˜æ ¼å¼é€šå¸¸ä¸ºï¼šå°åŒºå æˆ·å‹ é¢ç§¯
            # ä¾‹å¦‚ï¼š"ç”±ç”±ä¸€æ‘ 2å®¤1å… 64.24å¹³ç±³"
            community = None
            layout = None
            room_count = None
            hall_count = None
            area_sqm = None
            
            # æå–æˆ·å‹
            m_layout = re.search(r"(\d+å®¤\d+å…)", title_text)
            if m_layout:
                layout = m_layout.group(1)
                m_room = re.search(r"(\d+)å®¤", layout)
                m_hall = re.search(r"(\d+)å…", layout)
                if m_room:
                    room_count = int(m_room.group(1))
                if m_hall:
                    hall_count = int(m_hall.group(1))
                
                # å°åŒºåç§°åœ¨æˆ·å‹ä¹‹å‰
                community_part = title_text.split(layout)[0].strip()
                if community_part:
                    community = community_part
            else:
                # å¦‚æœæ²¡æœ‰æˆ·å‹ï¼Œå°è¯•ä»é¢ç§¯å‰æå–å°åŒºå
                m_area_temp = re.search(r"([\d.]+)å¹³ç±³", title_text)
                if m_area_temp:
                    community_part = title_text.split(m_area_temp.group(0))[0].strip()
                    if community_part:
                        community = community_part
            
            # æå–é¢ç§¯
            m_area = re.search(r"([\d.]+)å¹³ç±³", title_text)
            if m_area:
                try:
                    area_sqm = float(m_area.group(1))
                except Exception:
                    pass
            
            # æå–æœå‘ï¼ˆåœ¨mt18çš„pæ ‡ç­¾ä¸­ï¼‰
            orientation = None
            orient_elem = dd.find("p", class_="mt18")
            if orient_elem:
                orient_text = orient_elem.get_text(strip=True).split("|")[0].strip()
                if orient_text and orient_text.replace("å‘", "") in ["ä¸œ", "å—", "è¥¿", "åŒ—", "ä¸œå—", "è¥¿å—", "ä¸œåŒ—", "è¥¿åŒ—", "å—åŒ—"]:
                    orientation = orient_text.replace("å‘", "")
            
            # æ¥¼å±‚ä¿¡æ¯ï¼ˆæš‚æ—¶æ²¡æœ‰æ˜ç¡®æ ‡è¯†ï¼‰
            floor = None
            
            # æå–æˆäº¤æ—¥æœŸï¼ˆåœ¨area divä¸­çš„time pæ ‡ç­¾ï¼‰
            deal_date = None
            area_div = dd.find("div", class_="area")
            if area_div:
                time_elem = area_div.find("p", class_="time")
                if time_elem:
                    date_text = time_elem.get_text(strip=True)
                    if re.match(r"\d{4}-\d{2}-\d{2}", date_text):
                        deal_date = date_text
            
            # æå–ä»·æ ¼ï¼ˆåœ¨moreInfo divä¸­ï¼‰
            total_price_wan = None
            unit_price_yuan_sqm = None
            
            more_info = dd.find("div", class_="moreInfo")
            if more_info:
                # æ€»ä»·ï¼š<span class="price">1046</span>
                price_span = more_info.find("span", class_="price")
                if price_span:
                    try:
                        total_price_wan = float(price_span.get_text(strip=True))
                    except Exception:
                        pass
                
                # å•ä»·ï¼š<b>67415å…ƒ</b>
                danjia_p = more_info.find("p", class_="danjia")
                if danjia_p:
                    b_tag = danjia_p.find("b")
                    if b_tag:
                        unit_text = b_tag.get_text(strip=True).replace("å…ƒ", "")
                        try:
                            unit_price_yuan_sqm = int(unit_text)
                        except Exception:
                            pass
            
            # è¿‡æ»¤æ‰æ²¡æœ‰ä»·æ ¼çš„æ•°æ®
            if total_price_wan is None and unit_price_yuan_sqm is None:
                continue
            
            rows.append({
                "region": district_cn,
                "bizcircle": bizcircle_name,
                "community": community,
                "house_id": house_id,
                "detail_url": detail_url,
                "total_price_wan": total_price_wan,
                "unit_price_yuan_sqm": unit_price_yuan_sqm,
                "layout": layout,
                "room_count": room_count,
                "hall_count": hall_count,
                "area_sqm": area_sqm,
                "orientation": orientation,
                "building_year": None,
                "floor": floor,
                "deal_date": deal_date,
                "crawl_time": datetime.now().isoformat(timespec="seconds"),
            })
        except Exception:
            continue

    return rows


# ===========================
# æ ¸å¿ƒï¼šè·å–å¯ç”¨é¡µé¢
# ===========================
class EndOfDistrict(Exception):
    pass


def fetch_html_or_handle(
    session: requests.Session,
    url: str,
    context: str,
    page: int,
    all_data: List[Dict],
    stats: Dict[str, int],
) -> str:
    while True:
        resp = safe_get(session, url)
        if not resp or resp.status_code != 200:
            save_data(all_data)
            save_checkpoint(context, page, stats)
            raise RuntimeError("è¯·æ±‚å¤±è´¥æˆ–é200ï¼Œå·²ä¿å­˜æ•°æ®ä¸æ–­ç‚¹ã€‚")

        html = resp.text
        kind = classify_page(html, page)

        if kind == "OK":
            return html

        if kind == "END":
            dump_html("end_page", context, page, url, html)
            save_data(all_data)
            save_checkpoint(context, page, stats)
            raise EndOfDistrict()

        if kind == "VERIFY":
            print("  âš  è§¦å‘é£æ§/äººæœºéªŒè¯ï¼šéœ€è¦ä½ æ‰‹åŠ¨éªŒè¯åæ›´æ–° Cookieã€‚")
            dump_html("verify", context, page, url, html)
            save_data(all_data)
            save_checkpoint(context, page, stats)

            ok = prompt_cookie_and_update_session(session)
            if not ok:
                save_data(all_data)
                save_checkpoint(context, page, stats)
                raise SystemExit("æœªæä¾› Cookieï¼Œå·²ä¿å­˜æ•°æ®ä¸æ–­ç‚¹ï¼Œç¨‹åºç»“æŸã€‚")

            time.sleep(random.uniform(2.0, 4.0))
            continue

        dump_html("unknown_empty", context, page, url, html)
        save_data(all_data)
        save_checkpoint(context, page, stats)

        print("\n================ ç©ºé¡µ/å¼‚å¸¸é¡µï¼ˆæ— æ³•è‡ªåŠ¨åˆ¤æ–­ï¼‰ ================")
        print(f"URL: {url}")
        print("1) å¦‚æœä½ ç¡®è®¤è¿™æ˜¯é£æ§/éªŒè¯é¡µï¼šè¯·åœ¨æµè§ˆå™¨é€šè¿‡éªŒè¯åç²˜è´´æœ€æ–° Cookieï¼ˆå•è¡Œï¼‰")
        print("2) å¦‚æœä½ ç¡®è®¤è¯¥åŒºå·²ç»æ²¡æœ‰æ›´å¤šæˆ¿æºï¼šè¯·è¾“å…¥ END ç»“æŸè¯¥åŒº")
        print("3) ç›´æ¥å›è½¦ï¼šåœæ­¢ç¨‹åºï¼ˆå·²ä¿å­˜æ•°æ®ä¸æ–­ç‚¹ï¼‰")
        print("==========================================================\n")

        s = input("ç²˜è´´ Cookie / è¾“å…¥ END / å›è½¦åœæ­¢ > ").strip()
        if not s:
            raise SystemExit("ç”¨æˆ·åœæ­¢ï¼Œå·²ä¿å­˜æ•°æ®ä¸æ–­ç‚¹ã€‚")
        if s.upper() == "END":
            raise EndOfDistrict()

        cookie_dict = parse_manual_cookie_str(s)
        if not cookie_dict:
            print("âš  è¾“å…¥æ—¢ä¸æ˜¯ END ä¹Ÿä¸æ˜¯æœ‰æ•ˆ Cookieï¼Œå°†å†æ¬¡æç¤ºã€‚")
            continue

        try:
            session.cookies.clear()
        except Exception:
            pass
        session.cookies.update(cookie_dict)
        print(f"âœ… Cookie å·²æ›´æ–°ï¼šæ¡ç›®æ•° = {len(cookie_dict)}ï¼ˆå†…å®¹å·²éšè—ï¼‰")
        time.sleep(random.uniform(2.0, 4.0))


def get_bizcircle_page_url(district: str, bizcircle: str, page: int) -> str:
    """ç”Ÿæˆå•†åœˆæˆäº¤è®°å½•çš„åˆ†é¡µURL"""
    base = BIZCIRCLE_CHENGJIAO_URL.format(district=district, bizcircle=bizcircle)
    if page == 1:
        return base
    else:
        return base.rstrip('/') + f"/i3{page}/"


# ===========================
# ä¸»æµç¨‹ï¼šæŒ‰å•†åœˆçˆ¬å–å†å²æˆäº¤æ•°æ®
# ===========================
def main():
    print("=" * 60)
    print("æˆ¿å¤©ä¸‹å†å²æˆäº¤æ•°æ®çˆ¬è™« - æŒ‰å•†åœˆçˆ¬å–æ¨¡å¼")
    print("=" * 60)
    
    all_data: List[Dict] = load_json(OUTPUT_FILE, [])
    
    seen = set()
    stats: Dict[str, int] = {cn: 0 for cn, _ in DISTRICTS}
    for r in all_data:
        hid = r.get("house_id") or r.get("detail_url")
        if hid:
            seen.add(hid)
        cn = r.get("region")
        if cn in stats:
            stats[cn] += 1
    
    ck = load_json(CHECKPOINT_FILE, {})
    current_district = ck.get("current_district")
    current_bizcircle_idx = ck.get("current_bizcircle_idx", 0)
    current_page = ck.get("current_page", START_PAGE)
    
    session = build_session()
    
    print("\nå¦‚ä½ å·²å‡†å¤‡å¥½ Cookieï¼Œå¯ç›´æ¥ç²˜è´´ï¼ˆå›è½¦è·³è¿‡ï¼Œè§¦å‘éªŒè¯æ—¶å†ç²˜ï¼‰ï¼š")
    init_cookie = input("å¼€å±€ Cookieï¼ˆå¯ç©ºï¼‰> ").strip()
    if init_cookie:
        cdict = parse_manual_cookie_str(init_cookie)
        if cdict:
            session.cookies.update(cdict)
            print(f"âœ… å¼€å±€ Cookie å·²è®¾ç½®ï¼šæ¡ç›®æ•° = {len(cdict)}ï¼ˆå†…å®¹å·²éšè—ï¼‰")
        else:
            print("âš  å¼€å±€ Cookie è§£æå¤±è´¥ï¼Œå·²å¿½ç•¥ã€‚")
    
    try:
        for district_cn, district_slug in DISTRICTS:
            if current_district and district_cn != current_district:
                continue
            
            print(f"\n{'='*60}")
            print(f"å¼€å§‹çˆ¬å–åŒºåŸŸï¼š{district_cn}")
            print(f"{'='*60}")
            
            bizcircles_list = []
            bizcircle_file = BASE_DIR / f"bizcircles_{district_slug}.json"
            
            if bizcircle_file.exists():
                print(f"  ğŸ“‹ ä»ç¼“å­˜åŠ è½½å•†åœˆåˆ—è¡¨ï¼š{bizcircle_file.name}")
                bizcircles_list = load_json(bizcircle_file, [])
            else:
                print(f"  ğŸ” æ­£åœ¨è·å– {district_cn} çš„å•†åœˆåˆ—è¡¨...")
                try:
                    url = DISTRICT_CHENGJIAO_URL.format(district=district_slug)
                    resp = safe_get(session, url)
                    if resp and resp.status_code == 200:
                        bizcircles_list = get_bizcircles_from_district(resp.text, district_slug)
                        if bizcircles_list:
                            atomic_write_json(bizcircle_file, bizcircles_list)
                            print(f"  âœ… å…±è·å– {len(bizcircles_list)} ä¸ªå•†åœˆï¼Œå·²ç¼“å­˜")
                        else:
                            print(f"  âš  æœªèƒ½æå–åˆ° {district_cn} çš„å•†åœˆ")
                        polite_sleep()
                except Exception as e:
                    print(f"    âš  è·å–å•†åœˆåˆ—è¡¨å¤±è´¥ï¼š{e}")
            
            if not bizcircles_list:
                print(f"  âš  {district_cn} æœªè·å–åˆ°å•†åœˆåˆ—è¡¨ï¼Œè·³è¿‡")
                continue
            
            start_idx = current_bizcircle_idx if current_district == district_cn else 0
            bizcircles_to_crawl = bizcircles_list[start_idx:start_idx + TARGET_BIZCIRCLES_PER_DISTRICT]
            
            for idx, (bizcircle_name, bizcircle_code) in enumerate(bizcircles_to_crawl, start=start_idx):
                print(f"\n  [{district_cn}] å•†åœˆ {idx+1}/{len(bizcircles_list)}: {bizcircle_name} ({bizcircle_code})")
                
                empty_page_count = 0  # è¿ç»­ç©ºé¡µè®¡æ•°å™¨
                
                for page in range(START_PAGE, MAX_PAGES_PER_BIZCIRCLE + 1):
                    url = get_bizcircle_page_url(district_slug, bizcircle_code, page)
                    
                    try:
                        html = fetch_html_or_handle(session, url, f"{district_cn}_{bizcircle_name}", page, all_data, stats)
                    except EndOfDistrict:
                        print(f"    âœ… {bizcircle_name} ç¬¬{page}é¡µå·²æ— æ›´å¤šæˆäº¤è®°å½•")
                        break
                    except Exception as e:
                        print(f"    âš  ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ï¼š{e}")
                        break
                    
                    rows = parse_bizcircle_deals(html, bizcircle_name, district_cn)
                    if not rows:
                        empty_page_count += 1
                        print(f"    â„¹ ç¬¬{page}é¡µè§£æä¸åˆ°æ•°æ®ï¼ˆè¿ç»­ç©ºé¡µ: {empty_page_count}/{MAX_EMPTY_PAGES}ï¼‰")
                        
                        if empty_page_count >= MAX_EMPTY_PAGES:
                            print(f"    â›” è¿ç»­{MAX_EMPTY_PAGES}é¡µæ— æ•°æ®ï¼Œåœæ­¢è¯¥å•†åœˆ")
                            break
                        
                        polite_sleep()
                        continue  # ç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ
                    
                    # æœ‰æ•°æ®ï¼Œé‡ç½®ç©ºé¡µè®¡æ•°å™¨
                    empty_page_count = 0
                    
                    # æ£€æŸ¥å¹´ä»½è¿‡æ»¤
                    has_old_data = False  # æ˜¯å¦é‡åˆ°æ—©äºMIN_YEARçš„æ•°æ®
                    added = 0
                    filtered_by_year = 0
                    
                    for r in rows:
                        # æ£€æŸ¥å¹´ä»½
                        deal_date = r.get("deal_date")
                        if deal_date:
                            try:
                                deal_year = int(deal_date.split("-")[0])
                                
                                # å¦‚æœæ—©äºMIN_YEARï¼Œæ ‡è®°å¹¶åœæ­¢è¯¥å•†åœˆ
                                if deal_year < MIN_YEAR:
                                    has_old_data = True
                                    continue
                                
                                # å¦‚æœæ™šäºMAX_YEARï¼Œè·³è¿‡ä½†ç»§ç»­
                                if deal_year > MAX_YEAR:
                                    filtered_by_year += 1
                                    continue
                            except Exception:
                                pass  # æ—¥æœŸè§£æå¤±è´¥ï¼Œä¿ç•™è¯¥æ•°æ®
                        
                        # å»é‡
                        hid = r.get("house_id") or r.get("detail_url")
                        if hid and hid in seen:
                            continue
                        if hid:
                            seen.add(hid)
                        
                        all_data.append(r)
                        stats[district_cn] += 1
                        added += 1
                    
                    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                    if filtered_by_year > 0:
                        print(f"    ç¬¬ {page} é¡µï¼šè§£æ {len(rows)} æ¡ï¼Œè¿‡æ»¤ {filtered_by_year} æ¡ï¼ˆè¶…å‡ºå¹´ä»½ï¼‰ï¼Œæ–°å¢ {added} æ¡ï¼Œç´¯è®¡ {stats[district_cn]} æ¡")
                    else:
                        print(f"    ç¬¬ {page} é¡µï¼šè§£æ {len(rows)} æ¡ï¼Œæ–°å¢ {added} æ¡ï¼Œç´¯è®¡ {stats[district_cn]} æ¡")
                    
                    # å¦‚æœé‡åˆ°æ—©äºMIN_YEARçš„æ•°æ®ï¼Œåœæ­¢è¯¥å•†åœˆ
                    if has_old_data:
                        print(f"    â¹ æ£€æµ‹åˆ° {MIN_YEAR} å¹´ä¹‹å‰çš„æ•°æ®ï¼Œåœæ­¢è¯¥å•†åœˆ")
                        break
                    
                    if page % SAVE_EVERY_PAGES == 0:
                        save_data(all_data)
                        ck_data = {
                            "current_district": district_cn,
                            "current_bizcircle_idx": idx,
                            "current_page": page + 1,
                            "stats": stats,
                        }
                        atomic_write_json(CHECKPOINT_FILE, ck_data)
                    
                    polite_sleep()
                
                save_data(all_data)
            
            current_district = None
            current_bizcircle_idx = 0
            save_data(all_data)
            atomic_write_json(CHECKPOINT_FILE, {"stats": stats})
    
    except KeyboardInterrupt:
        print("\nâš  æ£€æµ‹åˆ° Ctrl+C æ‰“æ–­ï¼šæ­£åœ¨ä¿å­˜æ•°æ®ä¸æ–­ç‚¹...")
        save_data(all_data)
        print("âœ… å·²ä¿å­˜ã€‚ä¸‹æ¬¡ç›´æ¥é‡æ–°è¿è¡Œè„šæœ¬å³å¯æ–­ç‚¹ç»­çˆ¬ã€‚")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ‰ çˆ¬å–å®Œæˆ")
    print("=" * 60)
    print("å„åŒºæ•°é‡ï¼š", stats)
    print(f"æ€»è®¡ï¼š{sum(stats.values())} æ¡")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_FILE}")


if __name__ == "__main__":
    main()
